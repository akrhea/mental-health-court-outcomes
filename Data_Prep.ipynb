{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read in datasets from .csv's saved in same directory\n",
    "Change all column names to lowercase\n",
    "\n",
    "Initiation.csv downloaded Jan 5, 2020\n",
    "from https://datacatalog.cookcountyil.gov/Courts/Initiation/7mck-ehwz\n",
    "\n",
    "Dispositions.csv downloaded Jan 5, 2020\n",
    "from https://datacatalog.cookcountyil.gov/Courts/Dispositions/apwk-dzx8\n",
    "\n",
    "MHI_true.csv created via MHI_building.ipynb notebook\n",
    "'''\n",
    "\n",
    "initiation = pd.read_csv('Initiation.csv', low_memory=False)\n",
    "initiation.columns = [x.lower() for x in initiation.columns]\n",
    "\n",
    "dispositions = pd.read_csv('Dispositions.csv', low_memory=False)\n",
    "dispositions.columns = [x.lower() for x in dispositions.columns]\n",
    "\n",
    "sentencing = pd.read_csv('Dispositions.csv', low_memory=False)\n",
    "sentencing.columns = [x.lower() for x in sentencing.columns]\n",
    "\n",
    "MHI_true = pd.read_csv('MHI_true.csv', low_memory=False)\n",
    "MHI_true.columns = [x.lower() for x in MHI_true.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Filter initiation dataset \n",
    "Keep only the initiation records with\n",
    "case_participant_id's that exist in either sentencing or dispositions\n",
    "'''\n",
    "\n",
    "#save cp id values as lists\n",
    "init_ids = list(initiation.case_participant_id.values)\n",
    "dispo_ids = list(dispositions.case_participant_id.values)\n",
    "sent_ids = list(sentencing.case_participant_id.values)\n",
    "\n",
    "#create list of unique cp ids that are in both init & dispo\n",
    "ids_to_keep = list(set(init_ids).intersection(set(dispo_ids).union(set(sent_ids))))\n",
    "\n",
    "#create filtered df with only records from the intersection of cp ids\n",
    "init_filtered = initiation[initiation['case_participant_id'].isin(ids_to_keep)]\n",
    "\n",
    "del dispositions\n",
    "del sentencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new df to work on for processing/cleaning\n",
    "init_clean = init_filtered.copy()\n",
    "\n",
    "del init_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique age values:  [ 22.  nan  29.  34.  27.  41.  17.  20.  25.  59.  19.  43.  31.  23.\n",
      "  32.  18.  30.  21.  28.  39.  26.  46.  37.  44.  35.  49.  42.  24.\n",
      "  52.  51.  38.  33.  57.  58.  45.  48.  50.  65.  47.  53.  40.  60.\n",
      "  36.  56.  55.  67.  64.  54.  63.  70.  62.  61.  71.  66.  74.  75.\n",
      "  69.  68.  73.  72. 111.  85.  84.  86.  78.  77.  76.  79. 156.  81.\n",
      "  80.  82.  83. 112. 113. 114. 120. 127. 115. 130. 116.  96.  87. 117.\n",
      " 125. 118. 119.]\n",
      "Unique age values after cleaning:  [ 22   0  29  34  27  41  17  20  25  59  19  43  31  23  32  18  30  21\n",
      "  28  39  26  46  37  44  35  49  42  24  52  51  38  33  57  58  45  48\n",
      "  50  65  47  53  40  60  36  56  55  67  64  54  63  70  62  61  71  66\n",
      "  74  75  69  68  73  72 111  85  84  86  78  77  76  79 156  81  80  82\n",
      "  83 112 113 114 120 127 115 130 116  96  87 117 125 118 119]\n",
      "Number of records with null age:  10954\n",
      "Number of records with age > 100:  37\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Cleaning age_at_incident column\n",
    "'''\n",
    "\n",
    "#print unique values of age\n",
    "print('Unique age values: ', init_clean['age_at_incident'].unique())\n",
    "\n",
    "#turn nan to '0' in order to process into ints\n",
    "init_clean.loc[init_clean['age_at_incident'].isnull(), 'age_at_incident'] = '0'\n",
    "\n",
    "#change type to int\n",
    "init_clean.age_at_incident = init_clean.age_at_incident.astype(int)\n",
    "\n",
    "#look at unique values again\n",
    "print('Unique age values after cleaning: ', init_clean['age_at_incident'].unique())\n",
    "\n",
    "#how many values do we think are messed up?\n",
    "print('Number of records with null age: ', len(init_clean[init_clean['age_at_incident']==0]))\n",
    "print('Number of records with age > 100: ', len(init_clean[init_clean['age_at_incident']>100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with missing/incorrect age:  10991\n",
      "Original median age:  29.0\n",
      "Original mean age:  32.10448913979419\n",
      "New median age:  29.0\n",
      "New mean age:  32.058614903926305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelseymarkey/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create 2 new age-related binary features: age > 100, age_ unknown\n",
    "Use median to impute missing or presumed-incorrect age values\n",
    "'''\n",
    "\n",
    "#creating new binary columns for age over 100 and age unknown (1 = true)\n",
    "init_clean['age_over_100'] = (init_clean.age_at_incident > 100).astype(int) \n",
    "init_clean['age_unknown'] = (init_clean.age_at_incident == 0).astype(int)\n",
    "\n",
    "#replacing > 100 and 0 values with nan, calculating median, then changing nan to mean\n",
    "init_clean.age_at_incident.replace(0, np.NaN, inplace=True)\n",
    "a = np.array(init_clean['age_at_incident'].values.tolist())\n",
    "init_clean['age_at_incident'] = np.where(a > 100, np.nan, a).tolist()\n",
    "print('Number of records with missing/incorrect age: ', init_clean.age_at_incident.isna().sum())\n",
    "median = init_clean['age_at_incident'].median()\n",
    "print('Original median age: ', median)\n",
    "print('Original mean age: ', init_clean['age_at_incident'].mean())\n",
    "init_clean.age_at_incident.replace(np.NaN, median, inplace=True) \n",
    "print('New median age: ', median)\n",
    "print('New mean age: ', init_clean['age_at_incident'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert time features to datetime objects\n",
    "'''\n",
    "\n",
    "#list of time features\n",
    "time_features = ['event_date', 'incident_begin_date', 'arrest_date', \n",
    "                 'received_date', 'arraignment_date', 'incident_end_date']\n",
    "\n",
    "for col in time_features:\n",
    "    #turn nan to '01/01/1900 12:00:00 AM' so can process them as strings\n",
    "    init_clean.loc[init_clean[col].isnull(), col] = '01/01/1900 12:00:00 AM'\n",
    "\n",
    "    #make sure everything is strings\n",
    "    init_clean[col] = init_clean[col].astype(str)\n",
    "\n",
    "    #change type to datetime\n",
    "    init_clean[col] = init_clean[col].map(lambda x: pd.datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Male' 'Female' nan 'Male name, no gender given' 'Unknown Gender'\n",
      " 'Unknown']\n",
      "Number of records with gender=Male name, no gender given:  4\n",
      "Number of records with gender=Unknown:  7\n",
      "Number of records with gender=Unknown Gender:  2\n",
      "Number of records with null Gender:  2722\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Clean gender column\n",
    "'''\n",
    "\n",
    "#look at unique values\n",
    "print(init_clean['gender'].unique())\n",
    "\n",
    "#how many are messed up?\n",
    "print('Number of records with gender=Male name, no gender given: ', len(init_clean[init_clean['gender']=='Male name, no gender given']))\n",
    "print('Number of records with gender=Unknown: ', len(init_clean[init_clean['gender']=='Unknown']))\n",
    "print('Number of records with gender=Unknown Gender: ', len(init_clean[init_clean['gender']=='Unknown Gender']))\n",
    "print('Number of records with null Gender: ', len(init_clean[init_clean['gender'].isnull()]))\n",
    "\n",
    "#turning all except 'Male' and 'Female' to 'Unknown'\n",
    "init_clean.loc[init_clean['gender'].isnull(), 'gender'] = 'Unknown'\n",
    "init_clean.loc[init_clean['gender']=='Male name, no gender given', 'gender'] = 'Unknown'\n",
    "init_clean.loc[init_clean['gender']=='Unknown Gender', 'gender'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black' 'HISPANIC' 'White [Hispanic or Latino]'\n",
      " 'White/Black [Hispanic or Latino]' 'White' nan 'Unknown' 'Asian'\n",
      " 'Biracial' 'American Indian' 'Albino' 'ASIAN']\n",
      "['black' 'hispanic' 'white [hispanic or latino]'\n",
      " 'white/black [hispanic or latino]' 'white' nan 'unknown' 'asian'\n",
      " 'biracial' 'american indian' 'albino']\n",
      "black                               494039\n",
      "white [hispanic or latino]          128261\n",
      "white                                97785\n",
      "hispanic                              9259\n",
      "asian                                 5017\n",
      "white/black [hispanic or latino]      4011\n",
      "unknown                               1119\n",
      "american indian                        385\n",
      "biracial                               102\n",
      "albino                                   1\n",
      "Name: race, dtype: int64\n",
      "Number of records with null race:  3825\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Clean race column\n",
    "'''\n",
    "\n",
    "#look at unique values\n",
    "print(init_clean['race'].unique())\n",
    "\n",
    "#change all to lower, to combine ASIAN and Asian\n",
    "init_clean['race']=init_clean['race'].str.lower()\n",
    "\n",
    "#look at unique values again\n",
    "print(init_clean['race'].unique())\n",
    "\n",
    "#how many are messed up?\n",
    "print(init_clean['race'].value_counts())\n",
    "print('Number of records with null race: ', len(init_clean[init_clean['race'].isnull()]))\n",
    "\n",
    "#change nan to 'unknown'\n",
    "init_clean['race'].fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert charge_count to int\n",
    "'''\n",
    "\n",
    "#change type to int\n",
    "init_clean.charge_count = init_clean.charge_count.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert all non-id string features to lowercase\n",
    "'''\n",
    "\n",
    "string_cols = ['offense_category', 'charge_offense_title', 'chapter', 'act', \n",
    "               'section', 'class', 'aoic', 'event', 'gender', 'race', \n",
    "               'law_enforcement_agency', 'unit', 'incident_city', 'updated_offense_category']\n",
    "\n",
    "for col in string_cols:\n",
    "    init_clean[col]=init_clean[col].str.lower()\n",
    "    #note that str.lower() automatically ignores null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'section', 'event', 'law_enforcement_agency', 'unit', 'incident_city']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convert all remaining null values to 'unknown'\n",
    "'''\n",
    "\n",
    "#where are there still null values?\n",
    "print(init_clean.columns[init_clean.isna().any()].tolist())\n",
    "\n",
    "#all null values are in string columns\n",
    "#replace remaining null values with 'unknown' since this seems to be what Cook County uses\n",
    "init_clean.fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FEATURE ENGINEERING: Binary 402 Indicator Feature\n",
    "\n",
    "If an entry in section contains 402, then 402 column value will be 1. Else, it will be 0.\n",
    "\n",
    "'''\n",
    "# initialize empty column 402 \n",
    "init_clean[\"402\"] = 0\n",
    "\n",
    "def sectionconverter(x): \n",
    "    if \"402\" in x: \n",
    "        new_row = 1\n",
    "    else: \n",
    "        new_row = 0\n",
    "    return new_row\n",
    "\n",
    "init_clean['402'] = init_clean['section'].apply(sectionconverter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create dummy variables for categorical columns\n",
    "'''\n",
    "\n",
    "init_clean_copy = init_clean.copy()\n",
    "\n",
    "#create list of all categorical columns to get dummies for\n",
    "#excluding ID numbers, numerical features, datetime features, binary features\n",
    "cat_cols = ['offense_category', 'charge_offense_title', 'chapter', 'act', \n",
    "            'section', 'class', 'aoic', 'event', 'gender', 'race', \n",
    "            'law_enforcement_agency', 'unit', 'incident_city', 'updated_offense_category']\n",
    "dummy_init = pd.get_dummies(init_clean_copy, columns=cat_cols)\n",
    "\n",
    "del init_clean_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(743804, 5609)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Drop datetime features and the ID numbers we aren't using\n",
    "Datetime features will be reintroduced after aggregation\n",
    "\n",
    "List of time features defined in cleaning section:\n",
    "time_features = ['event_date', 'incident_begin_date', 'arrest_date', \n",
    "                 'received_date', 'arraignment_date', 'incident_end_date']\n",
    "'''\n",
    "\n",
    "#list of id features, including primary_charge flag\n",
    "id_features = ['case_id', 'primary_charge', 'charge_id', 'charge_version_id']\n",
    "\n",
    "to_drop = time_features + id_features\n",
    "dummy_init = dummy_init.drop(columns=to_drop)\n",
    "\n",
    "print(dummy_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Group by CP ID\n",
    "'''\n",
    "\n",
    "init_grouped = dummy_init.groupby('case_participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same:  ['case_id', 'case_participant_id', 'offense_category', 'event', 'event_date', 'age_at_incident', 'gender', 'race', 'incident_begin_date', 'incident_end_date', 'arrest_date', 'law_enforcement_agency', 'unit', 'incident_city', 'received_date', 'arraignment_date', 'updated_offense_category', 'age_over_100', 'age_unknown']\n",
      "different:  ['primary_charge', 'charge_id', 'charge_version_id', 'charge_offense_title', 'chapter', 'act', 'section', 'class', 'aoic', 'charge_count', '402']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preparing for aggregation\n",
    "Checking which features are the same for every unique CP ID\n",
    "'''\n",
    "\n",
    "clean_groups = init_clean.groupby('case_participant_id')\n",
    "same_within_group = []\n",
    "diff_within_group = []\n",
    "for col in init_clean.columns:\n",
    "    if any(clean_groups[col].nunique()>1):\n",
    "        diff_within_group.append(col)\n",
    "    else:\n",
    "        same_within_group.append(col)\n",
    "\n",
    "print('same: ', same_within_group)\n",
    "print('different: ', diff_within_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offense_category': 'median', 'event': 'median', 'age_at_incident': 'median', 'gender': 'median', 'race': 'median', 'law_enforcement_agency': 'median', 'unit': 'median', 'incident_city': 'median', 'updated_offense_category': 'median', 'age_over_100': 'median', 'age_unknown': 'median', 'charge_count': 'max', '402': 'max', 'charge_offense_title': 'sum', 'chapter': 'sum', 'act': 'sum', 'section': 'sum', 'class': 'sum', 'aoic': 'sum'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Creating dictionary for aggregation\n",
    "\n",
    "First, create \"short\" dictionary:\n",
    "Keys are columns in initiation, values are aggregation methods\n",
    "'''\n",
    "\n",
    "agg_dict_short = {}\n",
    "\n",
    "#we take the median for anything that's always the same within unique CP ID groups\n",
    "#excluding datetime features and case_id\n",
    "#note that every individual value should be equal to the median\n",
    "to_median = ['offense_category', 'event', 'age_at_incident', \n",
    "             'gender', 'race','law_enforcement_agency', 'unit', 'incident_city',\n",
    "             'updated_offense_category', 'age_over_100', 'age_unknown']\n",
    "agg_dict_short = {x : 'median' for x in to_median}\n",
    "\n",
    "#we take the highest charge_count to represent the total number of charges\n",
    "agg_dict_short['charge_count'] = 'max'\n",
    "\n",
    "#we take the highest \"402\" to maintain binary feature\n",
    "agg_dict_short['402'] = 'max'\n",
    "\n",
    "#list of categorical features that vary within unique case_participant_id groups, excluding id numbers\n",
    "to_sum = ['charge_offense_title', 'chapter', 'act', 'section', 'class', 'aoic']\n",
    "#we will sum these features when we aggregrate their dummies\n",
    "for item in to_sum:\n",
    "    agg_dict_short[item] = 'sum'\n",
    "\n",
    "print(agg_dict_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating dictionary for aggregation\n",
    "\n",
    "Use \"short\" dictionary to create \"long\" dictionary:\n",
    "Keys are columns in dummy_init, values are aggregation methods\n",
    "'''\n",
    "\n",
    "agg_dict_long = {}\n",
    "\n",
    "#loop through every key, val pair in agg_dict_short\n",
    "for key, val in agg_dict_short.items():\n",
    "\n",
    "    #check if key is not in the list of cat_cols that got turned into dummy variables\n",
    "    if key not in cat_cols:\n",
    "\n",
    "        #add key, val pair to agg_dict_long\n",
    "        agg_dict_long[key] = val\n",
    "\n",
    "    else:\n",
    "\n",
    "        #loop through every column in dummy_init\n",
    "        for col in dummy_init:\n",
    "\n",
    "            #check if the key is a substring of the dummy_init column name\n",
    "            if key+'_' in col:\n",
    "\n",
    "                #add val to agg_dict_long with the dummy_init column name as new key\n",
    "                agg_dict_long[col] = val\n",
    "\n",
    "del dummy_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aggregate the post-dummy groupby object\n",
    "'''\n",
    "\n",
    "init_squish = init_grouped.aggregate(agg_dict_long)\n",
    "# note that index is now cp id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive cases in aggregated dataset:  2212\n",
      "Number of negative cases in aggregated dataset:  286882\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Append MHI column to aggregated dataframe\n",
    "'''\n",
    "\n",
    "#initialize MHI column to false\n",
    "init_squish['MHI'] = 0\n",
    "\n",
    "#change MHI to 1 for the cp ids which appear in MHI_true\n",
    "init_squish.loc[init_squish.index.isin(MHI_true.case_participant_id.values), 'MHI']=1\n",
    "\n",
    "print('Number of positive cases in aggregated dataset: ', len(init_squish[init_squish['MHI']==1]))\n",
    "print('Number of negative cases in aggregated dataset: ', len(init_squish[init_squish['MHI']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Investigate discrepancy between number of positive cases \n",
    "in MHI_true vs. init_squish\n",
    "'''\n",
    "\n",
    "missing_positives = list(set(MHI_true.case_participant_id.values).difference(set(init_squish.index.values)))\n",
    "print(len(missing_positives))\n",
    "print(any(initiation.case_participant_id.isin(missing_positives)))\n",
    "#'False' confirms that none of the 'missing positives' are present in initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(289094, 5617)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Adding datetime features to aggregated dataframe\n",
    "Could not be aggregated because they are non-numeric\n",
    "Easily reintroduced because they are always the same within unique CP ID groups\n",
    "\n",
    "List of time features defined in cleaning section:\n",
    "time_features = ['event_date', 'incident_begin_date', 'arrest_date', \n",
    "                 'received_date', 'arraignment_date', 'incident_end_date']\n",
    "                 \n",
    "Also adding location-related categorical features\n",
    "As with time features, these are always the same within unique CP ID groups\n",
    "Currently duplicates the information from dummy variables\n",
    "Will be used for feature engineering, then removed\n",
    "'''\n",
    "\n",
    "#making df copies to work on\n",
    "total_df = init_squish.copy()\n",
    "time_df = init_clean.copy()\n",
    "\n",
    "#make list of columns to keep/delete from init_clean\n",
    "#keep time features, cp id, non-dummy location features\n",
    "to_keep = time_features + ['case_participant_id', 'incident_city', 'unit']\n",
    "to_delete  = list(set(time_df.columns) - set(to_keep))\n",
    "\n",
    "#drop columns in to_delete list\n",
    "time_df.drop(columns=to_delete, inplace=True)\n",
    "\n",
    "#drop rows with with duplicate cp id's\n",
    "time_df.drop_duplicates(subset='case_participant_id', keep='first', inplace=True)\n",
    "\n",
    "#confirm that time_df now has as many rows as init_squish\n",
    "print(len(init_squish)==len(time_df))\n",
    "#test works, prints True\n",
    "\n",
    "del init_squish\n",
    "del init_clean\n",
    "\n",
    "#set cp id to index of time_df\n",
    "time_df.set_index('case_participant_id', inplace=True)\n",
    "\n",
    "#join our time and location columns from time_df onto total_df\n",
    "total_df = total_df.join(time_df, on='case_participant_id')\n",
    "\n",
    "del time_df\n",
    "\n",
    "print(total_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_participant_id\n",
       "260122253823    0.0\n",
       "272161011760    1.0\n",
       "864286527653    1.0\n",
       "882206007016    1.0\n",
       "882242005211    0.0\n",
       "Name: weekday, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Creating a binary 'weekday' feature: 0 = Sat/Sun, 1 = M/T/W/Th/F\n",
    "'''\n",
    "\n",
    "#converting to datetime and replacing 1900 with NaT\n",
    "arrest_date_dt = pd.to_datetime(total_df.arrest_date) \n",
    "arrest_date_dt = arrest_date_dt.replace(datetime.datetime(1900, 1, 1, 0, 00), np.nan)\n",
    "\n",
    "#instantiating new column, converting NaT to np.nan, checking if weekday \n",
    "total_df['weekday'] = arrest_date_dt \n",
    "total_df['weekday'] = np.where(total_df.weekday.isnull(), np.nan, total_df.weekday.dt.dayofweek < 5)\n",
    "\n",
    "total_df.weekday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offense_category_aggravated assault police officer</th>\n",
       "      <th>offense_category_aggravated assault police officer firearm</th>\n",
       "      <th>offense_category_aggravated battery</th>\n",
       "      <th>offense_category_aggravated battery police officer</th>\n",
       "      <th>offense_category_aggravated battery police officer firearm</th>\n",
       "      <th>offense_category_aggravated battery with a firearm</th>\n",
       "      <th>offense_category_aggravated discharge firearm</th>\n",
       "      <th>offense_category_aggravated dui</th>\n",
       "      <th>offense_category_aggravated fleeing and eluding</th>\n",
       "      <th>offense_category_aggravated identity theft</th>\n",
       "      <th>...</th>\n",
       "      <th>arrest_date</th>\n",
       "      <th>unit</th>\n",
       "      <th>incident_city</th>\n",
       "      <th>received_date</th>\n",
       "      <th>arraignment_date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260122253823</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-05-22 18:51:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2011-05-24</td>\n",
       "      <td>2011-07-11 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272161011760</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009-07-14 14:34:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-01-27</td>\n",
       "      <td>1900-01-01 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864286527653</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-04-07 20:44:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>2010-05-19 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882206007016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-12-15 20:40:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>chicago</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>2011-01-31 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882242005211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2011-07-17 19:05:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>chicago</td>\n",
       "      <td>2011-07-17</td>\n",
       "      <td>1900-01-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5622 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     offense_category_aggravated assault police officer  \\\n",
       "case_participant_id                                                       \n",
       "260122253823                                                         0    \n",
       "272161011760                                                         0    \n",
       "864286527653                                                         0    \n",
       "882206007016                                                         0    \n",
       "882242005211                                                         0    \n",
       "\n",
       "                     offense_category_aggravated assault police officer firearm  \\\n",
       "case_participant_id                                                               \n",
       "260122253823                                                         0            \n",
       "272161011760                                                         0            \n",
       "864286527653                                                         0            \n",
       "882206007016                                                         0            \n",
       "882242005211                                                         0            \n",
       "\n",
       "                     offense_category_aggravated battery  \\\n",
       "case_participant_id                                        \n",
       "260122253823                                           0   \n",
       "272161011760                                           0   \n",
       "864286527653                                           0   \n",
       "882206007016                                           0   \n",
       "882242005211                                           0   \n",
       "\n",
       "                     offense_category_aggravated battery police officer  \\\n",
       "case_participant_id                                                       \n",
       "260122253823                                                         0    \n",
       "272161011760                                                         0    \n",
       "864286527653                                                         0    \n",
       "882206007016                                                         0    \n",
       "882242005211                                                         0    \n",
       "\n",
       "                     offense_category_aggravated battery police officer firearm  \\\n",
       "case_participant_id                                                               \n",
       "260122253823                                                         0            \n",
       "272161011760                                                         0            \n",
       "864286527653                                                         0            \n",
       "882206007016                                                         0            \n",
       "882242005211                                                         0            \n",
       "\n",
       "                     offense_category_aggravated battery with a firearm  \\\n",
       "case_participant_id                                                       \n",
       "260122253823                                                         0    \n",
       "272161011760                                                         0    \n",
       "864286527653                                                         0    \n",
       "882206007016                                                         0    \n",
       "882242005211                                                         0    \n",
       "\n",
       "                     offense_category_aggravated discharge firearm  \\\n",
       "case_participant_id                                                  \n",
       "260122253823                                                     0   \n",
       "272161011760                                                     0   \n",
       "864286527653                                                     0   \n",
       "882206007016                                                     0   \n",
       "882242005211                                                     0   \n",
       "\n",
       "                     offense_category_aggravated dui  \\\n",
       "case_participant_id                                    \n",
       "260122253823                                       0   \n",
       "272161011760                                       0   \n",
       "864286527653                                       0   \n",
       "882206007016                                       0   \n",
       "882242005211                                       0   \n",
       "\n",
       "                     offense_category_aggravated fleeing and eluding  \\\n",
       "case_participant_id                                                    \n",
       "260122253823                                                       0   \n",
       "272161011760                                                       0   \n",
       "864286527653                                                       0   \n",
       "882206007016                                                       0   \n",
       "882242005211                                                       0   \n",
       "\n",
       "                     offense_category_aggravated identity theft  ...  \\\n",
       "case_participant_id                                              ...   \n",
       "260122253823                                                  0  ...   \n",
       "272161011760                                                  0  ...   \n",
       "864286527653                                                  0  ...   \n",
       "882206007016                                                  0  ...   \n",
       "882242005211                                                  0  ...   \n",
       "\n",
       "                            arrest_date     unit  incident_city  \\\n",
       "case_participant_id                                               \n",
       "260122253823        2011-05-22 18:51:00  unknown        unknown   \n",
       "272161011760        2009-07-14 14:34:00  unknown        unknown   \n",
       "864286527653        2010-04-07 20:44:00  unknown        unknown   \n",
       "882206007016        2010-12-15 20:40:00  unknown        chicago   \n",
       "882242005211        2011-07-17 19:05:00  unknown        chicago   \n",
       "\n",
       "                     received_date     arraignment_date  weekday  season_fall  \\\n",
       "case_participant_id                                                             \n",
       "260122253823            2011-05-24  2011-07-11 00:00:00      0.0            0   \n",
       "272161011760            2012-01-27  1900-01-01 00:00:00      1.0            0   \n",
       "864286527653            2011-01-31  2010-05-19 00:00:00      1.0            0   \n",
       "882206007016            2011-01-31  2011-01-31 00:00:00      1.0            1   \n",
       "882242005211            2011-07-17  1900-01-01 00:00:00      0.0            0   \n",
       "\n",
       "                     season_spring  season_summer  season_winter  \n",
       "case_participant_id                                               \n",
       "260122253823                     1              0              0  \n",
       "272161011760                     0              1              0  \n",
       "864286527653                     1              0              0  \n",
       "882206007016                     0              0              0  \n",
       "882242005211                     0              1              0  \n",
       "\n",
       "[5 rows x 5622 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Creating a 'season' feature: spring, summer, fall, winter\n",
    "\n",
    "'''\n",
    "from datetime import date, datetime\n",
    "\n",
    "arrest_date_dt = pd.to_datetime(total_df.arrest_date) #converting to datetime\n",
    "total_df['day_of_year'] = arrest_date_dt.dt.dayofyear #making day of year column with ordinal date\n",
    "\n",
    "#checking for season\n",
    "total_df['season'] = np.where(total_df['day_of_year'] <= 80, 'winter', total_df['day_of_year'])\n",
    "total_df['season'] = np.where((173 > total_df['day_of_year']) &  (total_df['day_of_year'] > 80), 'spring', total_df['season'])\n",
    "total_df['season'] = np.where((267 > total_df['day_of_year']) &  (total_df['day_of_year'] >= 173), 'summer', total_df['season'])\n",
    "total_df['season'] = np.where((356 > total_df['day_of_year']) &  (total_df['day_of_year'] >= 267), 'fall', total_df['season'])\n",
    "total_df['season'] = np.where(356 <= total_df['day_of_year'], 'winter', total_df['season'])\n",
    "\n",
    "total_df = total_df.drop(columns = 'day_of_year') #dropping day of year column  \n",
    "total_df = pd.get_dummies(total_df, columns = ['season']) #create dummy variables\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    279416\n",
       "1      1998\n",
       "2       394\n",
       "3       245\n",
       "4       197\n",
       "Name: incident_length, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Creating an 'incident length' feature which is the difference between the incident begin and end date. \n",
    "If either is null, then incident length = 0.\n",
    "'''\n",
    "\n",
    "#convert begin and end dates to datetime\n",
    "begin_date_dt = pd.to_datetime(total_df.incident_begin_date)\n",
    "begin_date_dt = begin_date_dt.replace(datetime(1900, 1, 1, 0, 00), np.nan) #replacing 1900 with NaT\n",
    "end_date_dt = pd.to_datetime(total_df.incident_end_date)\n",
    "end_date_dt = end_date_dt.replace(datetime(1900, 1, 1, 0, 00), np.nan) #replacing 1900 with NaT\n",
    "\n",
    "#create incident_length column by subtracting, this will be NaT if either is NaT\n",
    "total_df['incident_length'] = (end_date_dt - begin_date_dt)\n",
    "\n",
    "#replace NaT with 0 \n",
    "total_df['incident_length'] = np.where(total_df.incident_length.isnull(), 0, total_df.incident_length)\n",
    "\n",
    "#convert all timestamps to days \n",
    "total_df['incident_length'] = total_df.incident_length.dt.days\n",
    "\n",
    "total_df.incident_length.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RateLimiter caught an error, retrying (0/2 tries). Called with (*('palatine, Illinois',), **{}).\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 1317, in do_open\n",
      "    encode_chunked=req.has_header('Transfer-encoding'))\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 1229, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 1275, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 1224, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 1016, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 956, in send\n",
      "    self.connect()\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/http/client.py\", line 1392, in connect\n",
      "    server_hostname=server_hostname)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/ssl.py\", line 412, in wrap_socket\n",
      "    session=session\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/ssl.py\", line 853, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/ssl.py\", line 1117, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "socket.timeout: _ssl.c:1039: The handshake operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/site-packages/geopy/geocoders/base.py\", line 355, in _call_geocoder\n",
      "    page = requester(req, timeout=timeout, **kwargs)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 543, in _open\n",
      "    '_open', req)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 1360, in https_open\n",
      "    context=self._context, check_hostname=self._check_hostname)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/urllib/request.py\", line 1319, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error _ssl.c:1039: The handshake operation timed out>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/site-packages/geopy/extra/rate_limiter.py\", line 126, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/site-packages/geopy/geocoders/osm.py\", line 387, in geocode\n",
      "    self._call_geocoder(url, timeout=timeout), exactly_one\n",
      "  File \"/Users/kelseymarkey/anaconda3/lib/python3.7/site-packages/geopy/geocoders/base.py\", line 378, in _call_geocoder\n",
      "    raise GeocoderTimedOut('Service timed out')\n",
      "geopy.exc.GeocoderTimedOut: Service timed out\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Geoencoding incident city to create latitude and longitude columns.\n",
    "'''\n",
    "\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "#creating array of unique cities and adding state to city names in list\n",
    "uniqueCities = total_df['incident_city'].unique()\n",
    "uniquePlaces =  [(i + \", Illinois\") for i in uniqueCities]\n",
    "#drop Unknown and convert to df\n",
    "uniquePlaces_df = pd.DataFrame(uniquePlaces[1:])\n",
    "\n",
    "#geoencode unique list\n",
    "#1 - convenient function to delay between geocoding calls\n",
    "locator = Nominatim(user_agent='myGeocoder')\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n",
    "#2 - create location column\n",
    "uniquePlaces_df['location'] = uniquePlaces_df[0].apply(geocode)\n",
    "#3 - create longitude, laatitude and altitude from location column (returns tuple)\n",
    "uniquePlaces_df['point'] = uniquePlaces_df['location'].apply(lambda loc: tuple(loc.point) if loc else None)\n",
    "#4 - split tuple into longitude and latitude (and altitude)\n",
    "uniquePlaces_df[['latitude', 'longitude', 'altitude']] = pd.DataFrame(uniquePlaces_df['point'].tolist(), index=uniquePlaces_df.index)\n",
    "\n",
    "#remove state from city names\n",
    "uniquePlaces_df[0]=uniquePlaces_df[0].str[:-10]\n",
    "\n",
    "#set default values (will remain 0 if incident_city is unknown)\n",
    "total_df['latitude'] = 0\n",
    "total_df['longitude'] = 0\n",
    "\n",
    "#assign appropriate long/lat\n",
    "for i in range(len(uniquePlaces_df)):\n",
    "    samePlace = uniquePlaces_df[0][i]\n",
    "    lat = uniquePlaces_df['latitude'][i]\n",
    "    lon = uniquePlaces_df['longitude'][i]\n",
    "    \n",
    "    #update lat and lon if df['incident_city']==uniquePlaces_df['0']\n",
    "    total_df['latitude'] = np.where((total_df.incident_city == samePlace), lat, total_df.latitude)\n",
    "    total_df['longitude'] = np.where((total_df.incident_city == samePlace), lon, total_df.longitude)\n",
    "\n",
    "del uniquePlaces_df\n",
    "\n",
    "total_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('total_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KM Notes: \n",
    "- Does it look weird that we create new df and then delete? I know this is an artifact of before when we exported datasets at intermediary stages, but now that its all combined should we fix. Perhaps not because the dataframes get renamed to more applicable things at each step? \n",
    "- Check later if we reintroduce primary_charge?? Could be interesting to keep in but we delete in second cell in groupby/aggregate section\n",
    "- In first cell of Feature Engineering the Weekday column is floats. Do they need to be ints? Might be hard with NaNs\n",
    "\n",
    "\n",
    "*** RERUN FROM START ONCE DONE!!! CHECK SEASON HEAD!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
