{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from Final_Data_Prep import remove_final_dummy, get_train_test, downsample\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in fresh copy of df\n",
    "df = pd.read_pickle('total_df.pckl.gz', compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Prep (consistent with ADS as seen in Final_Gradient_Boosting_Model.ipynb)\n",
    "\n",
    "Performing prep on total_df (here called df), not decoded_df as above in LIME\n",
    "'''\n",
    "\n",
    "#put earliest 85% of cases into training df, latest 15% of cases into test df\n",
    "training, test = get_train_test(df, train_size=0.85, test_size=0.15)\n",
    "\n",
    "# passing 50 to downsample function means training set will have 50% positive cases\n",
    "training = downsample(training, 50)\n",
    "\n",
    "X_train = training.drop('MHI', axis=1)\n",
    "y_train = training['MHI']\n",
    "\n",
    "X_test = test.drop('MHI', axis=1)\n",
    "y_test = test['MHI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_total = GradientBoostingClassifier(random_state=42, max_depth = 3, \n",
    "                                       min_samples_leaf = 10, learning_rate=.01, \n",
    "                                       n_estimators=1000)\n",
    "clf_total.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions with original classifier\n",
    "y_pred = clf_total.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 289094 to array axis with dimension 5616",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cf8530bcc10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'received_date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MHI'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_all_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# initialize 2-d predictions arrays to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 289094 to array axis with dimension 5616"
     ]
    }
   ],
   "source": [
    "# set classifier list:\n",
    "list_clf = [clf_total]\n",
    "\n",
    "# get number of classifiers\n",
    "num_clf = len(list_clf)\n",
    "\n",
    "# make stacked array of predictions on test set\n",
    "preds_test = np.array([y_pred])\n",
    "\n",
    "# set X and y for full dataset\n",
    "y_all = df['MHI'].copy()\n",
    "X_all = df.drop(columns=['received_date', 'MHI'], inplace=False)\n",
    "\n",
    "X_all_stack = np.array([X_all])\n",
    "\n",
    "# initialize 2-d predictions arrays to 0\n",
    "preds_all = np.zeros([num_clf, len(y_all)])\n",
    "\n",
    "# get predictions on whole dataset\n",
    "for i, clf in enumerate(list_clf):\n",
    "\n",
    "    # get predictions on entire dataset\n",
    "    preds_all[i] = clf.predict(X_all_stack[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def di_actual_and_pred(X, y_true, y_pred, col, priv_class):\n",
    "    '''\n",
    "    Calculate disparate impact in actual data and in classifier predictions\n",
    "    DI = Pr(MHI==1|non-privileged class) / Pr(MHI==1| privileged class)\n",
    "       = (# positive outcomes in the non-privileged class / # non-privileged)\n",
    "           / (# positive outcomes in the privileged class / # privileged)\n",
    "\n",
    "    X: features\n",
    "    y_true: actual labels\n",
    "    y_pred: 2-d array of labels predicted by different classifiers\n",
    "    col: category (e.g. race, gender)\n",
    "    priv_class: privileged class (e.g. white, male)\n",
    "    '''\n",
    "    \n",
    "    # get number of classifiers\n",
    "    num_clf = y_pred.shape[0]\n",
    "\n",
    "    # set privileged column\n",
    "    priv_col = col+'_'+priv_class\n",
    "\n",
    "    # get number of privileged\n",
    "    priv = X[X[priv_col]==1]\n",
    "    num_priv = len(priv)\n",
    "\n",
    "    # get number of privileged with actual positive\n",
    "    ind_priv = priv.index\n",
    "    y_priv = y_true.loc[ind_priv]\n",
    "    true_num_pos_priv = y_priv.sum()\n",
    "\n",
    "    # get number of non-privileged\n",
    "    nonpriv = X[X[priv_col]==0]\n",
    "    num_nonpriv = len(nonpriv)\n",
    "\n",
    "    # get number of non-privileged with actual positive\n",
    "    ind_nonpriv = nonpriv.index\n",
    "    y_nonpriv = y_true.loc[ind_nonpriv]\n",
    "    true_num_pos_nonpriv = y_nonpriv.sum()\n",
    "    \n",
    "    # get ilocs for actual positives in each class\n",
    "    priv_ilocs = y_true.index.get_indexer_for((ind_priv))\n",
    "    nonpriv_ilocs = y_true.index.get_indexer_for((ind_nonpriv))\n",
    "\n",
    "    # get DI in original data\n",
    "    DI_original = (true_num_pos_nonpriv / num_nonpriv) / (true_num_pos_priv / num_priv)\n",
    "    print('Disparate impact on {} in original data: {}'.format(col, round(DI_original, 4)))\n",
    "    \n",
    "    # initialize DI_pred to array of 1s\n",
    "    DI_pred = np.ones([num_clf])\n",
    "    \n",
    "    # calculate DI in predictions for each classifier\n",
    "    for i in range(num_clf):\n",
    "        \n",
    "        # get number of privileged with positive prediction\n",
    "        pred_num_pos_priv = y_pred[i][priv_ilocs].sum()\n",
    "\n",
    "        # get number of non-privileged with positive prediction\n",
    "        pred_num_pos_nonpriv = y_pred[i][nonpriv_ilocs].sum()\n",
    "\n",
    "        # get DI in predictions\n",
    "        DI_pred[i] = (pred_num_pos_nonpriv / num_nonpriv) / (pred_num_pos_priv / num_priv)\n",
    "        print('Disparate impact on {} in classifier {} predictions: {}'.format(col, i+1, round(DI_pred[i], 4)))\n",
    "    \n",
    "    return DI_original, DI_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DI measures\n",
    "\n",
    "print('Test Set')\n",
    "di_race_og_test, di_race_pred_test = di_actual_and_pred(X_test, y_test, preds_test, 'race', 'white')\n",
    "di_gender_og_test, di_gender_pred_test = di_actual_and_pred(X_test, y_test, preds_test, 'gender', 'male')\n",
    "\n",
    "print('\\nEntire Dataset')\n",
    "di_race_og_all, di_race_pred_all = di_actual_and_pred(X_all, y_all, preds_all, 'race', 'white')\n",
    "di_gender_og_all, di_gender_pred_all = di_actual_and_pred(X_all, y_all, preds_all, 'gender', 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add dummy data to actual from above\n",
    "# # actual data will come from proxy-sensitive classifiers\n",
    "\n",
    "# di_race_og_test = 0.5719\n",
    "# di_race_pred_test = 0.5192\n",
    "# di_race_og_all = 0.4569\n",
    "# di_race_pred_all = 0.5289\n",
    "\n",
    "# di_gender_og_test = 2.4247\n",
    "# di_gender_pred_test = 3.2531\n",
    "# di_gender_og_all = 3.5604\n",
    "# di_gender_pred_all = 3.1462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put DI measures into dataframe\n",
    "\n",
    "\n",
    "multi_index = pd.MultiIndex.from_product([['Test Set', 'All Data'],\n",
    "                                          ['Actual',\n",
    "                                           'Predictions']])\n",
    "\n",
    "race_list = [di_race_og_test, di_race_pred_test[0], \n",
    "             di_race_og_all, di_race_pred_all[0]]\n",
    "\n",
    "gender_list = [di_gender_og_test, di_gender_pred_test[0],\n",
    "               di_gender_og_all, di_gender_pred_all[0]]\n",
    "\n",
    "\n",
    "di_df = pd.DataFrame(data={'Race':[round(x, 4) for x in race_list], \n",
    "                           'Gender':[round(x, 4) for x in gender_list]}, \n",
    "                     index=multi_index)\n",
    "\n",
    "di_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_melt = di_df.copy()\n",
    "di_melt['Dataset'] = di_melt.index.map(lambda x: str(x)[1:-1].replace(\"\\'\", \"\"))\n",
    "di_melt = di_melt.melt(id_vars=['Dataset'], \n",
    "                  value_vars=['Race', 'Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot DI\n",
    "\n",
    "# Create an array with the colors for plot\n",
    "colors = ['#142733', '#1d4159', '#23577a', '#4a7d9e', '#231c2e', '#3b2f52', '#594b73', '#85799c']\n",
    "\n",
    "di_plot = sns.catplot(data=di_melt, \n",
    "                          x='variable', y='value', hue='Dataset', palette=sns.color_palette(colors),\n",
    "                          kind='bar', aspect=2, height=7, legend_out=False)\n",
    "\n",
    "plt.tick_params(axis='x', which='both',bottom=False, top=False)\n",
    "di_plot.axes[0][0].set_xlabel('')\n",
    "di_plot.axes[0][0].set_xticklabels(['Race', 'Gender'], fontsize=16)\n",
    "di_plot.axes[0][0].set_ylabel(r'$\\frac{Pr(MHI=1|non-privileged)}{Pr(MHI=1| privileged)}$', \n",
    "                              fontsize=24)\n",
    "\n",
    "di_plot.axes[0][0].set_axisbelow(True)\n",
    "\n",
    "plt.axhline(1, color='black', linestyle='dotted', linewidth=2, label='Equal Impact')\n",
    "plt.axhline(0.8, color='#b8b8b8', linestyle='dotted', linewidth=2, label='80% Impact / Inverse Impact')\n",
    "plt.axhline(1.25, color='#b8b8b8', linestyle='dotted', linewidth=2)\n",
    "\n",
    "plt.legend(title='Dataset', fontsize=12, title_fontsize=14)\n",
    "plt.title('Disparate Impact', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(X_sub, col):\n",
    "    \n",
    "    # get target variable subset\n",
    "    ind = X_sub.index\n",
    "    y_sub = y_test[ind]\n",
    "    \n",
    "    # for subset, get predictions and probabilities for positive class \n",
    "    pred_sub = clf_total.predict(X_sub)\n",
    "    pred_proba_sub = clf_total.predict_proba(X_sub)\n",
    "    \n",
    "    # get FPR, TPR\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_sub, pred_proba_sub[:,1])\n",
    "    \n",
    "    # get confusion matrix\n",
    "    confmat_sub = sklearn.metrics.confusion_matrix(y_sub, pred_sub, labels=[1,0])\n",
    "    tp = confmat_sub[0,0] # number of true positives\n",
    "    fp = confmat_sub[1,0] # number of false positives\n",
    "    fn = confmat_sub[0,1] # number of false negatives\n",
    "    tn = confmat_sub[1,1] # number of true negatives\n",
    "    \n",
    "    # create dataframe with stats\n",
    "    stats = pd.DataFrame(data={col:[len(pred_sub), # group size\n",
    "                                    round(sum(pred_sub)/len(pred_sub), 4), #pred prev\n",
    "                                    round((tp+tn)/len(pred_sub), 4), # accuracy\n",
    "                                    round(fn/(fn+tp), 4), # fnr\n",
    "                                    round(fp/(fp+tn), 4), # fpr\n",
    "                                    round(tp/(tp+fn), 4), # recall\n",
    "                                    round(sklearn.metrics.auc(fpr, tpr), 4) # auc \n",
    "                                   ]}, \n",
    "                 index=['Test Set Group Size (N)', 'Predicted Prevalence', \n",
    "                        'Accuracy', 'FNR', 'FPR', 'Recall', 'AUC'])\n",
    "    # transpose df\n",
    "    stats = stats.T\n",
    "    stats['Test Set Group Size (N)'] = stats['Test Set Group Size (N)'].astype(int)\n",
    "    \n",
    "    # display dataframe with stats\n",
    "    display(stats)\n",
    "    \n",
    "    #plot confusion matrix\n",
    "    sklearn.metrics.plot_confusion_matrix(clf_total, X_sub, y_sub, labels=[1,0])\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    # return dataframe with stats\n",
    "    return stats\n",
    "    \n",
    "    \n",
    "def subgroup_stats(category1, category2=None):\n",
    "    '''\n",
    "    Takes in 1 or 2 pre-OHE column names (e.g., 'race' and 'gender').\n",
    "    If 2 categories given, produces intersectional statistics.\n",
    "    \n",
    "    Note: Does not support intersectional analysis with age\n",
    "    '''\n",
    "    # get list of OHE columns for category 1\n",
    "    if category1 != 'age_at_incident':\n",
    "        cols_1 = [x for x in X_test.columns if x.startswith(category1+'_')]\n",
    "        \n",
    "    # get overall stats\n",
    "    total_stats = get_stats(X_test, 'Overall')\n",
    "    \n",
    "    # intersectional analysis\n",
    "    if category2 !=None:\n",
    "        assert category1!='age_at_incident' and category2!='age_at_incident', \\\n",
    "                                         'Intersectional analysis not supported for age'\n",
    "        \n",
    "        # get list of OHE columns for category 1\n",
    "        cols_2 = [x for x in X_test.columns if x.startswith(category2+'_')]\n",
    "        \n",
    "        for x in cols_1:\n",
    "            # for each value of category 1, create subset\n",
    "            X_sub = X_test[X_test[x]==1]\n",
    "            \n",
    "            # continue if subset is empty\n",
    "            if len(X_sub)==0:\n",
    "                print(x,'\\n\\t No members of this subgroup present in the test set. \\n')\n",
    "                continue\n",
    "                \n",
    "            # if category 1 subset not empty, create sub-subsets\n",
    "            else:\n",
    "            \n",
    "                for y in cols_2:\n",
    "                    # create sub-subset for each value of category 2\n",
    "                    X_sub = X_test[X_test[x]==1]\n",
    "                    X_sub = X_sub[X_sub[y]==1]\n",
    "                    if len(X_sub)==0:\n",
    "                        print(x+' and '+y,\n",
    "                              '\\n\\t No members of this subgroup present in the test set. \\n')\n",
    "                    else:\n",
    "                        # get stats if sub-subset not empty\n",
    "                        sub_stats = get_stats(X_sub, x+' and '+y)\n",
    "                        total_stats = pd.concat([total_stats, sub_stats])\n",
    "                        \n",
    "    # single trait only\n",
    "    else:\n",
    "        if category1 != 'age_at_incident':\n",
    "            for x in cols_1:\n",
    "                # for each value of category 1, create subset\n",
    "                X_sub = X_test[X_test[x]==1]\n",
    "                if len(X_sub)==0:\n",
    "                    print(x,'\\n\\t No members of this subgroup present in the test set. \\n')\n",
    "                else:\n",
    "                    # get stats if subset not empty\n",
    "                    sub_stats = get_stats(X_sub, x)\n",
    "                    total_stats = pd.concat([total_stats, sub_stats])\n",
    "            \n",
    "        # special method for age\n",
    "        else:\n",
    "            # cut age into bins\n",
    "            cut = pd.cut(X_test['age_at_incident'], \n",
    "                         bins=[X_test['age_at_incident'].min(), \n",
    "                               20, 23, 27, 30, 35, 40, 50, 60, 70, \n",
    "                               X_test['age_at_incident'].max()+1], \n",
    "                         labels=False, right=False, retbins=True)\n",
    "            \n",
    "            # save age_at_incident encoded by bin numbers\n",
    "            age_binned = cut[0]\n",
    "            \n",
    "            # save bins\n",
    "            bins = cut[1]\n",
    "            \n",
    "            for x in sorted(age_binned.unique()):\n",
    "                # create subset for each bin\n",
    "                this_bin = age_binned[age_binned==x].index\n",
    "                X_sub = X_test.loc[this_bin]\n",
    "                \n",
    "                # create bin labels\n",
    "                edges = '{}-{}'.format(int(bins[int(x)]), int(bins[int(x)+1]-1))\n",
    "                if bins[int(x)] == bins[int(x)+1]-1:\n",
    "                    # if bin contains only 1 age, change label to single number\n",
    "                    edges = str(int(bins[int(x)]))\n",
    "            \n",
    "                if len(X_sub)==0:\n",
    "                    if not np.isnan(x):\n",
    "                        print(x,\n",
    "                              '\\n\\t No members of this subgroup present in the test set. \\n')\n",
    "                else:\n",
    "                    # get stats if subset not empty\n",
    "                    sub_stats = get_stats(X_sub, edges)\n",
    "                    total_stats = pd.concat([total_stats, sub_stats])\n",
    "                    \n",
    "    display(total_stats)\n",
    "    print('\\n')\n",
    "    \n",
    "    return total_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_group(x):\n",
    "    \n",
    "    split = x.split('_', 1)\n",
    "    if len(split)==1:\n",
    "        return split[0].title()\n",
    "    if len(split)==2:\n",
    "        return split[1].title()\n",
    "    return\n",
    "\n",
    "def plot_stats(stat_df_full, age=False):\n",
    "    \n",
    "    stat_df = stat_df_full.iloc[1:]\n",
    "    \n",
    "    if not age:\n",
    "        # Set category (e.g. \"Gender\")\n",
    "        cat = stat_df.index[-1].split('_', 1)[0].title()\n",
    "\n",
    "        # Create column with category values and test set size\n",
    "        stat_df[cat] = stat_df.index.map(plain_group) + ', N=' + \\\n",
    "                            stat_df['Test Set Group Size (N)'].astype(str)\n",
    "\n",
    "        stat_df = stat_df.sort_values(by=['Predicted Prevalence'])\n",
    "        \n",
    "        # Set palette\n",
    "        palette='muted'\n",
    "    \n",
    "    if age:\n",
    "        # Set category to \"Age\"\n",
    "        cat = 'Age'\n",
    "        \n",
    "        # Create column with category values and test set size\n",
    "        stat_df[cat] = stat_df.index + ', N=' + \\\n",
    "                            stat_df['Test Set Group Size (N)'].astype(str)\n",
    "        \n",
    "        # Set sequential palette\n",
    "        palette = 'GnBu'\n",
    "\n",
    "    # melt df to prepare for sns.catplot\n",
    "    df_melt = stat_df.melt(id_vars=[cat], \n",
    "                  value_vars=['Predicted Prevalence', 'Accuracy', \n",
    "                              'FNR', 'FPR', 'Recall', 'AUC'])\n",
    "\n",
    "    # plot metrics\n",
    "    stat_plot = sns.catplot(data=df_melt, \n",
    "                              x='variable', y='value', hue=cat,\n",
    "                              kind='bar', palette=palette,\n",
    "                            aspect=2, height=6, legend_out=False)\n",
    "    \n",
    "    stat_plot.axes[0][0].set_xlabel('')\n",
    "    stat_plot.axes[0][0].set_ylabel('')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_stats = subgroup_stats('age_at_incident')\n",
    "plot_stats(age_stats, age=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stats = subgroup_stats('gender')\n",
    "plot_stats(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_stats = subgroup_stats('race')\n",
    "plot_stats(race_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_and_gender_stats = subgroup_stats('race', 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
